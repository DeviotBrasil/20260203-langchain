{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fd1df8db",
   "metadata": {},
   "source": [
    "# Aula 006 - Memória de Conversação com RunnableWithMessageHistory\n",
    "\n",
    "Este notebook demonstra como adicionar memória de conversação a uma cadeia LCEL\n",
    "usando RunnableWithMessageHistory, permitindo que o modelo mantenha contexto\n",
    "entre múltiplas interações.\n",
    "\n",
    "**Conceitos abordados:**\n",
    "- ChatPromptTemplate com placeholder para histórico\n",
    "- InMemoryChatMessageHistory: Armazenamento de mensagens em memória\n",
    "- RunnableWithMessageHistory: Wrapper que adiciona memória a cadeias\n",
    "- Gerenciamento de sessões para múltiplas conversas\n",
    "- Operador pipe (|): Sintaxe LCEL para encadeamento\n",
    "\n",
    "**Caso de uso:**\n",
    "- Chatbot de recomendação de cidades turísticas\n",
    "- Conversa multi-turno com contexto preservado\n",
    "\n",
    "**Fluxo da cadeia:**\n",
    "```\n",
    "{query, historico} -> ChatPromptTemplate -> ChatOpenAI -> StrOutputParser -> string\n",
    "                           ↑\n",
    "             RunnableWithMessageHistory (gerencia histórico por sessão)\n",
    "```\n",
    "\n",
    "**Dependências:**\n",
    "- langchain-openai: Integração do LangChain com OpenAI\n",
    "- langchain-core: Output parsers, histórico de mensagens\n",
    "- python-dotenv: Gerenciamento de variáveis de ambiente"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a66d8eb2",
   "metadata": {},
   "source": [
    "## Importações e Configuração"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5100ad61",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.chat_history import InMemoryChatMessageHistory\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "from langchain.globals import set_debug\n",
    "\n",
    "# Carrega as variáveis de ambiente do arquivo .env\n",
    "load_dotenv()\n",
    "\n",
    "# Obtém a chave da API\n",
    "api_key = os.getenv('OPENAI_API_KEY')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "859816a1",
   "metadata": {},
   "source": [
    "## Criação do Modelo e do Prompt com Histórico\n",
    "\n",
    "O placeholder `{historico}` será automaticamente preenchido com as mensagens anteriores da conversa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa0b2577",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inicialização do modelo\n",
    "modelo = ChatOpenAI(\n",
    "    model_name=\"gpt-3.5-turbo\",\n",
    "    openai_api_key=api_key,\n",
    "    temperature=0.7,\n",
    "    max_tokens=500\n",
    ")\n",
    "\n",
    "# Criação do prompt com suporte a histórico de conversa\n",
    "prompt_sugestao = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"Você é um assistente especializado em recomendar cidades turísticas com base nos interesses do usuário.\"),\n",
    "        (\"placeholder\", \"{historico}\"),\n",
    "        (\"human\", \"{query}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Criação da cadeia base\n",
    "cadeia = prompt_sugestao | modelo | StrOutputParser()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da9806e4",
   "metadata": {},
   "source": [
    "## Configuração da Memória de Conversação\n",
    "\n",
    "- `memoria`: Dicionário que armazena o histórico por sessão\n",
    "- `historico_por_sessao`: Função que retorna o histórico de uma sessão específica\n",
    "- `RunnableWithMessageHistory`: Wrapper que gerencia automaticamente o histórico"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22935a23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dicionário para armazenar o histórico de cada sessão\n",
    "memoria = {}\n",
    "sessao_id = \"aula006_sessao1\"\n",
    "\n",
    "def historico_por_sessao(sessao_id: str) -> InMemoryChatMessageHistory:\n",
    "    \"\"\"Retorna o histórico de mensagens para uma sessão específica.\"\"\"\n",
    "    if sessao_id not in memoria:\n",
    "        memoria[sessao_id] = InMemoryChatMessageHistory()\n",
    "    return memoria[sessao_id]\n",
    "\n",
    "# Cria a cadeia com memória\n",
    "cadeia_com_memoria = RunnableWithMessageHistory(\n",
    "    runnable=cadeia,\n",
    "    get_session_history=historico_por_sessao,\n",
    "    input_messages_key=\"query\",\n",
    "    history_messages_key=\"historico\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da591ba5",
   "metadata": {},
   "source": [
    "## Conversa Multi-turno\n",
    "\n",
    "Observe como a segunda pergunta faz referência à primeira resposta, demonstrando que o modelo mantém o contexto da conversa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "078d749b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lista de perguntas para simular uma conversa multi-turno\n",
    "lista_pergunta = [\n",
    "    \"Quero visitar cidades com muitas praias. Quais você recomenda?\",\n",
    "    \"Qual a melhor epoca do ano para visitar essas cidades?\",\n",
    "]\n",
    "\n",
    "for pergunta in lista_pergunta:\n",
    "    resposta = cadeia_com_memoria.invoke(\n",
    "        {\n",
    "            \"query\": pergunta\n",
    "        },\n",
    "        config={\"session_id\": sessao_id}\n",
    "    )\n",
    "\n",
    "    print(\"Usuario: \", pergunta)\n",
    "    print(\"Assistente: \", resposta, \"\\n\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
